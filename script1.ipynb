{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the necessary libraries/modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data do some odservations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv('restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This place has the best dumplings I've ever ha...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Came here on a weekend visit to NY, my friend ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The best soup dumplings in NYC! There is no ot...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was so disappointed with my visit to Joes. N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So much fun!  we stayed at Hotel 50\\r\\nRight n...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  This place has the best dumplings I've ever ha...       5\n",
       "1  Came here on a weekend visit to NY, my friend ...       4\n",
       "2  The best soup dumplings in NYC! There is no ot...       5\n",
       "3  I was so disappointed with my visit to Joes. N...       1\n",
       "4  So much fun!  we stayed at Hotel 50\\r\\nRight n...       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7275</th>\n",
       "      <td>Soup dumplings! Best i ever had. Get them, oth...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7276</th>\n",
       "      <td>my favorite soup dumpling place.\\r\\r\\nive been...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7277</th>\n",
       "      <td>Its cash only and there is usually a line but ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7278</th>\n",
       "      <td>The crab and pork soup dumplings are great.  T...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7279</th>\n",
       "      <td>So-so food... I am not coming back to crappy s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review  Rating\n",
       "7275  Soup dumplings! Best i ever had. Get them, oth...       4\n",
       "7276  my favorite soup dumpling place.\\r\\r\\nive been...       5\n",
       "7277  Its cash only and there is usually a line but ...       5\n",
       "7278  The crab and pork soup dumplings are great.  T...       3\n",
       "7279  So-so food... I am not coming back to crappy s...       3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7280 entries, 0 to 7279\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  7280 non-null   object\n",
      " 1   Rating  7280 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 113.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [missing, percentage]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a data frame showing missing values and their percentage\n",
    "missing = pd.DataFrame(df.isnull().sum(), columns = ['missing'])\n",
    "missing['percentage'] = missing['missing'] / df.shape[0] * 100\n",
    "missing = missing[missing['missing'] > 0]\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the duplicate rows and remove if any\n",
    "df.duplicated().sum()\n",
    "\n",
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7270.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.834801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.178673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rating\n",
       "count  7270.000000\n",
       "mean      3.834801\n",
       "std       1.178673\n",
       "min       1.000000\n",
       "25%       3.000000\n",
       "50%       4.000000\n",
       "75%       5.000000\n",
       "max       5.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets do descriptive statistics \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Rating'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets explore the distribution of target variable\n",
    "df['Rating'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 7270\n",
      "Rating 5\n"
     ]
    }
   ],
   "source": [
    "# lets check the unique values of each column\n",
    "for col in df.columns:\n",
    "    print(col, df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do some data cleaning\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to clean the text data\n",
    "def clean_text(text):\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # convert text to lower case\n",
    "    text = text.lower()\n",
    "    # remove extra whitespaces\n",
    "    text = text.strip()\n",
    "    # remove extra tabs\n",
    "    text = text.replace('\\t', ' ')\n",
    "    # remove extra newlines\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "    # lemmatize the text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word, pos = 'v') for word in words]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>place best dumplings ive ever family go want f...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>come weekend visit ny friend recommend spot de...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best soup dumplings nyc place compare delicate...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disappoint visit joes restaurant accept cash a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much fun stay hotel right next door joes first...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  place best dumplings ive ever family go want f...       5\n",
       "1  come weekend visit ny friend recommend spot de...       4\n",
       "2  best soup dumplings nyc place compare delicate...       5\n",
       "3  disappoint visit joes restaurant accept cash a...       1\n",
       "4  much fun stay hotel right next door joes first...       5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a word cloud to visualize the most frequent words in the reviews\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = set(stopwords.words('english')),\n",
    "                min_font_size = 20).generate(' '.join(df['Review']))\n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create multiple word clouds for each sentiment using for loop\n",
    "ratings = df['Rating'].unique()\n",
    "for rating in ratings:\n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = set(stopwords.words('english')),\n",
    "                min_font_size = 10).generate(' '.join(df[df['Rating'] == rating]['Review']))\n",
    "\n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.title(rating)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets preprocess the data for modeling\n",
    "lem = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [lem.lemmatize(word, pos = 'v') for word in words]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "df['Review'] = df['Review'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lets split the data into train and test sets\n",
    "# X = df['Review']\n",
    "# y = df['Rating']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lets create a function to train and evaluate the model\n",
    "# def train_evaluate_model(X_train, X_test, y_train, y_test, vectorizer, classifier):\n",
    "#     # vectorize the text data\n",
    "#     X_train = vectorizer.fit_transform(X_train)\n",
    "#     X_test = vectorizer.transform(X_test)\n",
    "#     # train the model\n",
    "#     classifier.fit(X_train, y_train)\n",
    "#     # make predictions\n",
    "#     y_pred = classifier.predict(X_test)\n",
    "#     # evaluate the model\n",
    "#     print('Accuracy:\\n', accuracy_score(y_test, y_pred))\n",
    "#     print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "#     print('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lets use naive bayes classifier\n",
    "# vectorizer = CountVectorizer()\n",
    "# classifier = MultinomialNB()\n",
    "# train_evaluate_model(X_train, X_test, y_train, y_test, vectorizer, classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "# classifier = PassiveAggressiveClassifier()\n",
    "# train_evaluate_model(X_train, X_test, y_train, y_test, vectorizer, classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = vectorizer.fit_transform(X_train)\n",
    "# X_test = vectorizer.transform(X_test)\n",
    "# # train the model\n",
    "# classifier.fit(X_train, y_train)\n",
    "# # make predictions\n",
    "# y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets try different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a new feature 'Length' and 'Sentiment'\n",
    "df['Length'] = df['Review'].apply(len)\n",
    "df['Sentiment'] = df['Rating'].apply(lambda x: 'Positive' if x >= 4 else 'Negative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Length</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>place best dumplings ive ever family go want f...</td>\n",
       "      <td>5</td>\n",
       "      <td>564</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>come weekend visit ny friend recommend spot de...</td>\n",
       "      <td>4</td>\n",
       "      <td>469</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best soup dumplings nyc place compare delicate...</td>\n",
       "      <td>5</td>\n",
       "      <td>290</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disappoint visit joes restaurant accept cash a...</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much fun stay hotel right next door joes first...</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating  Length Sentiment\n",
       "0  place best dumplings ive ever family go want f...       5     564  Positive\n",
       "1  come weekend visit ny friend recommend spot de...       4     469  Positive\n",
       "2  best soup dumplings nyc place compare delicate...       5     290  Positive\n",
       "3  disappoint visit joes restaurant accept cash a...       1     360  Negative\n",
       "4  much fun stay hotel right next door joes first...       5     200  Positive"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets create a model to predict the sentiment\n",
    "X = df['Review']\n",
    "y = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function to train and evaluate the model\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test, vectorizer, classifier):\n",
    "    # vectorize the text data\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    # lets encode the target variable\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    # train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    # evaluate the model\n",
    "    print('Accuracy:\\n', accuracy_score(y_test, y_pred))\n",
    "    print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "    print('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer: CountVectorizer()\n",
      "Classifier: MultinomialNB()\n",
      "Accuracy:\n",
      " 0.838147638697845\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.63      0.70       647\n",
      "           1       0.85      0.93      0.89      1534\n",
      "\n",
      "    accuracy                           0.84      2181\n",
      "   macro avg       0.82      0.78      0.79      2181\n",
      "weighted avg       0.83      0.84      0.83      2181\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 405  242]\n",
      " [ 111 1423]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: CountVectorizer()\n",
      "Classifier: PassiveAggressiveClassifier()\n",
      "Accuracy:\n",
      " 0.7932141219624026\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.67      0.66       647\n",
      "           1       0.86      0.84      0.85      1534\n",
      "\n",
      "    accuracy                           0.79      2181\n",
      "   macro avg       0.75      0.76      0.75      2181\n",
      "weighted avg       0.80      0.79      0.79      2181\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 434  213]\n",
      " [ 238 1296]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: CountVectorizer()\n",
      "Classifier: LogisticRegression()\n",
      "Accuracy:\n",
      " 0.8376891334250344\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.69      0.71       647\n",
      "           1       0.87      0.90      0.89      1534\n",
      "\n",
      "    accuracy                           0.84      2181\n",
      "   macro avg       0.81      0.79      0.80      2181\n",
      "weighted avg       0.83      0.84      0.84      2181\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 444  203]\n",
      " [ 151 1383]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: CountVectorizer()\n",
      "Classifier: RandomForestClassifier()\n",
      "Accuracy:\n",
      " 0.8023842274186153\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.39      0.54       647\n",
      "           1       0.79      0.98      0.87      1534\n",
      "\n",
      "    accuracy                           0.80      2181\n",
      "   macro avg       0.83      0.68      0.71      2181\n",
      "weighted avg       0.82      0.80      0.77      2181\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 251  396]\n",
      " [  35 1499]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: CountVectorizer()\n",
      "Classifier: SVC()\n",
      "Accuracy:\n",
      " 0.8276020174232004\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.53      0.64       647\n",
      "           1       0.83      0.96      0.89      1534\n",
      "\n",
      "    accuracy                           0.83      2181\n",
      "   macro avg       0.83      0.74      0.77      2181\n",
      "weighted avg       0.83      0.83      0.81      2181\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 340  307]\n",
      " [  69 1465]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: TfidfVectorizer()\n",
      "Classifier: MultinomialNB()\n",
      "Accuracy:\n",
      " 0.7198532783127006\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.06      0.11       647\n",
      "           1       0.72      1.00      0.83      1534\n",
      "\n",
      "    accuracy                           0.72      2181\n",
      "   macro avg       0.84      0.53      0.47      2181\n",
      "weighted avg       0.79      0.72      0.62      2181\n",
      "\n",
      "Confusion Matrix: \n",
      " [[  37  610]\n",
      " [   1 1533]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: TfidfVectorizer()\n",
      "Classifier: PassiveAggressiveClassifier()\n",
      "Accuracy:\n",
      " 0.8000917010545622\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       647\n",
      "           1       0.86      0.86      0.86      1534\n",
      "\n",
      "    accuracy                           0.80      2181\n",
      "   macro avg       0.76      0.76      0.76      2181\n",
      "weighted avg       0.80      0.80      0.80      2181\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 426  221]\n",
      " [ 215 1319]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: TfidfVectorizer()\n",
      "Classifier: LogisticRegression()\n",
      "Accuracy:\n",
      " 0.8477762494268684\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.61      0.70       647\n",
      "           1       0.85      0.95      0.90      1534\n",
      "\n",
      "    accuracy                           0.85      2181\n",
      "   macro avg       0.84      0.78      0.80      2181\n",
      "weighted avg       0.85      0.85      0.84      2181\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 392  255]\n",
      " [  77 1457]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: TfidfVectorizer()\n",
      "Classifier: RandomForestClassifier()\n",
      "Accuracy:\n",
      " 0.8110958276020174\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.41      0.57       647\n",
      "           1       0.80      0.98      0.88      1534\n",
      "\n",
      "    accuracy                           0.81      2181\n",
      "   macro avg       0.84      0.70      0.72      2181\n",
      "weighted avg       0.83      0.81      0.79      2181\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 268  379]\n",
      " [  33 1501]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: TfidfVectorizer()\n",
      "Classifier: SVC()\n",
      "Accuracy:\n",
      " 0.8500687757909215\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.62      0.71       647\n",
      "           1       0.86      0.95      0.90      1534\n",
      "\n",
      "    accuracy                           0.85      2181\n",
      "   macro avg       0.84      0.78      0.80      2181\n",
      "weighted avg       0.85      0.85      0.84      2181\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 401  246]\n",
      " [  81 1453]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets create a loop to create different models\n",
    "vectorizers = [CountVectorizer(), TfidfVectorizer()]\n",
    "classifiers = [MultinomialNB(), PassiveAggressiveClassifier(), LogisticRegression(), RandomForestClassifier(), SVC()]\n",
    "for vectorizer in vectorizers:\n",
    "    for classifier in classifiers:\n",
    "        print('Vectorizer:', vectorizer)\n",
    "        print('Classifier:', classifier)\n",
    "        train_evaluate_model(X_train, X_test, y_train, y_test, vectorizer, classifier)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Positive    2236\n",
       "Negative    2236\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract the same number of positive and negative reviews\n",
    "positive = df[df['Sentiment'] == 'Positive'].sample(2236)\n",
    "negative = df[df['Sentiment'] == 'Negative'].sample(2236)\n",
    "df = pd.concat([positive, negative])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Positive    2236\n",
       "Negative    2236\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do train test split\n",
    "X = df['Review']\n",
    "y = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer: CountVectorizer()\n",
      "Classifier: MultinomialNB()\n",
      "Accuracy:\n",
      " 0.8256333830104322\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82       651\n",
      "           1       0.83      0.83      0.83       691\n",
      "\n",
      "    accuracy                           0.83      1342\n",
      "   macro avg       0.83      0.83      0.83      1342\n",
      "weighted avg       0.83      0.83      0.83      1342\n",
      "\n",
      "Confusion Matrix: \n",
      " [[537 114]\n",
      " [120 571]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: CountVectorizer()\n",
      "Classifier: PassiveAggressiveClassifier()\n",
      "Accuracy:\n",
      " 0.7801788375558867\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.78       651\n",
      "           1       0.80      0.77      0.78       691\n",
      "\n",
      "    accuracy                           0.78      1342\n",
      "   macro avg       0.78      0.78      0.78      1342\n",
      "weighted avg       0.78      0.78      0.78      1342\n",
      "\n",
      "Confusion Matrix: \n",
      " [[517 134]\n",
      " [161 530]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: CountVectorizer()\n",
      "Classifier: LogisticRegression()\n",
      "Accuracy:\n",
      " 0.8047690014903129\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80       651\n",
      "           1       0.81      0.81      0.81       691\n",
      "\n",
      "    accuracy                           0.80      1342\n",
      "   macro avg       0.80      0.80      0.80      1342\n",
      "weighted avg       0.80      0.80      0.80      1342\n",
      "\n",
      "Confusion Matrix: \n",
      " [[523 128]\n",
      " [134 557]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: CountVectorizer()\n",
      "Classifier: RandomForestClassifier()\n",
      "Accuracy:\n",
      " 0.7906110283159463\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.79       651\n",
      "           1       0.82      0.77      0.79       691\n",
      "\n",
      "    accuracy                           0.79      1342\n",
      "   macro avg       0.79      0.79      0.79      1342\n",
      "weighted avg       0.79      0.79      0.79      1342\n",
      "\n",
      "Confusion Matrix: \n",
      " [[532 119]\n",
      " [162 529]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: CountVectorizer()\n",
      "Classifier: SVC()\n",
      "Accuracy:\n",
      " 0.7988077496274217\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       651\n",
      "           1       0.82      0.78      0.80       691\n",
      "\n",
      "    accuracy                           0.80      1342\n",
      "   macro avg       0.80      0.80      0.80      1342\n",
      "weighted avg       0.80      0.80      0.80      1342\n",
      "\n",
      "Confusion Matrix: \n",
      " [[532 119]\n",
      " [151 540]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: TfidfVectorizer()\n",
      "Classifier: MultinomialNB()\n",
      "Accuracy:\n",
      " 0.8330849478390462\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       651\n",
      "           1       0.85      0.81      0.83       691\n",
      "\n",
      "    accuracy                           0.83      1342\n",
      "   macro avg       0.83      0.83      0.83      1342\n",
      "weighted avg       0.83      0.83      0.83      1342\n",
      "\n",
      "Confusion Matrix: \n",
      " [[555  96]\n",
      " [128 563]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: TfidfVectorizer()\n",
      "Classifier: PassiveAggressiveClassifier()\n",
      "Accuracy:\n",
      " 0.786140089418778\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.80      0.78       651\n",
      "           1       0.80      0.77      0.79       691\n",
      "\n",
      "    accuracy                           0.79      1342\n",
      "   macro avg       0.79      0.79      0.79      1342\n",
      "weighted avg       0.79      0.79      0.79      1342\n",
      "\n",
      "Confusion Matrix: \n",
      " [[520 131]\n",
      " [156 535]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: TfidfVectorizer()\n",
      "Classifier: LogisticRegression()\n",
      "Accuracy:\n",
      " 0.8211624441132638\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       651\n",
      "           1       0.84      0.81      0.82       691\n",
      "\n",
      "    accuracy                           0.82      1342\n",
      "   macro avg       0.82      0.82      0.82      1342\n",
      "weighted avg       0.82      0.82      0.82      1342\n",
      "\n",
      "Confusion Matrix: \n",
      " [[544 107]\n",
      " [133 558]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: TfidfVectorizer()\n",
      "Classifier: RandomForestClassifier()\n",
      "Accuracy:\n",
      " 0.7891207153502235\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79       651\n",
      "           1       0.81      0.77      0.79       691\n",
      "\n",
      "    accuracy                           0.79      1342\n",
      "   macro avg       0.79      0.79      0.79      1342\n",
      "weighted avg       0.79      0.79      0.79      1342\n",
      "\n",
      "Confusion Matrix: \n",
      " [[525 126]\n",
      " [157 534]]\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer: TfidfVectorizer()\n",
      "Classifier: SVC()\n",
      "Accuracy:\n",
      " 0.8248882265275708\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       651\n",
      "           1       0.84      0.82      0.83       691\n",
      "\n",
      "    accuracy                           0.82      1342\n",
      "   macro avg       0.82      0.83      0.82      1342\n",
      "weighted avg       0.83      0.82      0.82      1342\n",
      "\n",
      "Confusion Matrix: \n",
      " [[543 108]\n",
      " [127 564]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets create a loop to create different models\n",
    "vectorizers = [CountVectorizer(), TfidfVectorizer()]\n",
    "classifiers = [MultinomialNB(), PassiveAggressiveClassifier(), LogisticRegression(), RandomForestClassifier(), SVC()]\n",
    "for vectorizer in vectorizers:\n",
    "    for classifier in classifiers:\n",
    "        print('Vectorizer:', vectorizer)\n",
    "        print('Classifier:', classifier)\n",
    "        train_evaluate_model(X_train, X_test, y_train, y_test, vectorizer, classifier)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.5}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lest use MultinomialNB and LogisticRegression with TfidfVectorizer and do hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "vectorizer = TfidfVectorizer()\n",
    "classifier = MultinomialNB()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "params = {'alpha': [0.1, 0.5, 1, 5, 10]}\n",
    "grid = GridSearchCV(classifier, param_grid = params, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:32: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:32: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\A S U S\\AppData\\Local\\Temp\\ipykernel_9120\\2475100309.py:32: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub('\\s+', ' ', text)  # Replace multiple whitespace characters with a single space\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import streamlit as st\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tokenizer=word_tokenize):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = []\n",
    "        for text in X:\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            text = re.sub('[^a-zA-Z]', ' ', text)  # Keep only lowercase and uppercase letters\n",
    "            text = re.sub('\\s+', ' ', text)  # Replace multiple whitespace characters with a single space\n",
    "            text = self.tokenizer(text)  # Tokenize the text\n",
    "            X_transformed.append(text)\n",
    "        return X_transformed\n",
    "\n",
    "class TextVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vectorizer=TfidfVectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = self.vectorizer.transform(X)\n",
    "        return X_transformed\n",
    "    \n",
    "class SentimentClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classifier=LogisticRegression):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classifier.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classifier.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(X, y)\n",
    "    \n",
    "class SentimentPipeline:\n",
    "    def __init__(self, classifier=SentimentClassifier()):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classifier.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('text_cleaner', TextPreprocessor),\n",
    "    ('vectorizer', TextVectorizer),\n",
    "    ('classifier', SentimentClassifier),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nlp_pipeline.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(pipeline,'Nlp_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
